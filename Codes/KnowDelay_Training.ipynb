{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1939,
     "status": "ok",
     "timestamp": 1627135937900,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "8chtxwy8EhnL",
    "outputId": "f3705570-b24e-4944-b593-50f7900cfcd5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "error",
     "timestamp": 1627135938277,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "lXftTJu5EhnO",
    "outputId": "c104dbf6-ef02-48d4-eb17-02eebd5a2746"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "from imblearn.over_sampling import (RandomOverSampler, SMOTENC)\n",
    "from sklearn.linear_model import (ElasticNet, Ridge, Lasso, LogisticRegression)\n",
    "from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "import json\n",
    "from category_encoders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "aborted",
     "timestamp": 1627135938258,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "EApEm9RoEhnP"
   },
   "outputs": [],
   "source": [
    "models_dct={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1627135938260,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "De8ceZKuEhnP"
   },
   "outputs": [],
   "source": [
    "def get_training_data(X,Y):\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test =train_test_split(X,Y, test_size=0.20, random_state=42)\n",
    "    print('X Train',X_train.shape)\n",
    "    print('X test',X_test.shape)\n",
    "    print('Y Train',Y_train.shape)\n",
    "    print('Y test',Y_test.shape)\n",
    "    #X_train,Y_train = train[:,0:cols-2],train[:,cols-1]\n",
    "    #X_test,Y_test = test[:,0:cols-2],test[:,cols-1]\n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1627135938263,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "0eeIuIyiEhnP"
   },
   "outputs": [],
   "source": [
    "def classification_metrics(Y_pred,Y_true):\n",
    "    cnf_matrix = metrics.confusion_matrix(Y_true, Y_pred)\n",
    "    accuracy=metrics.accuracy_score(Y_true, Y_pred)\n",
    "    bal_accuracy= metrics.balanced_accuracy_score(Y_true, Y_pred)\n",
    "    precision=metrics.precision_score(Y_true, Y_pred,average='weighted')\n",
    "    recall=metrics.recall_score(Y_true, Y_pred,average='weighted')\n",
    "    fscore=metrics.f1_score(Y_true, Y_pred,average='weighted')\n",
    "    #auc = metrics.roc_auc_score(Y_true, Y_pred,average='micro')\n",
    "    return accuracy,precision,recall,fscore,bal_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1627135938269,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "l7pAOeLuEhnQ"
   },
   "outputs": [],
   "source": [
    "def display_metrics(classifierName,Y_pred,Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    if np.array_equal(Y_pred,Y_true):\n",
    "        print(\"Match\")\n",
    "        \n",
    "    acc, precision, recall, f1score,bal_accuracy = classification_metrics(Y_pred,Y_true)\n",
    "    mc_points = Y_pred==Y_true\n",
    "    mc_p=np.size(mc_points) - np.count_nonzero(mc_points)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    #print((\"AUC: \"+str(auc_)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print((\"Balanced Accuracy: \"+str(bal_accuracy)))\n",
    "    #print((\"Misclassified Points: \"+str(mc_p)))\n",
    "    \n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "aborted",
     "timestamp": 1627135938270,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "Mh_gBUY4EhnQ"
   },
   "outputs": [],
   "source": [
    "def AlphaGridSearch(X_train,Y_train):\n",
    "    print('Performing Grid Searching for best alpha...')\n",
    "    cvLasso = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "    lasso = Lasso(max_iter = 1e5, normalize = True)\n",
    "    grid = dict()\n",
    "    grid['alpha'] = [1e-5, 1e-3, 1e-2, 1, 10]\n",
    "    # define search\n",
    "    search = GridSearchCV(lasso, grid, scoring='neg_mean_absolute_error', cv=cvLasso, n_jobs=-1)\n",
    "    # perform the search\n",
    "    results = search.fit(X_train, Y_train)\n",
    "    alpha = results.best_params_['alpha']\n",
    "    return alpha\n",
    "\n",
    "def LGGridSearch(X_train,Y_train):\n",
    "    print('Performing Grid Searching for best gamma and labmda...')\n",
    "    cvEnet = RepeatedKFold(n_splits=5, n_repeats = 3, random_state = 1)\n",
    "    enet = ElasticNet(max_iter = 1e3, normalize = True)\n",
    "    # define grid\n",
    "    grid = dict()\n",
    "    grid['alpha'] = [1e-2, 1e-1, 1.0, 10.0]\n",
    "    grid['l1_ratio'] = [0, 0.5, 1.]\n",
    "    search = GridSearchCV(enet, grid, scoring='neg_mean_absolute_error', cv=cvEnet, n_jobs=-1)\n",
    "    # perform the search\n",
    "    results_enet = search.fit(X_train, Y_train)\n",
    "    \n",
    "    lambda_enet = results_enet.best_params_['alpha']\n",
    "    gamma_enet = results_enet.best_params_['l1_ratio']\n",
    "\n",
    "    return lambda_enet, gamma_enet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1627135938270,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "VczOgDbGEhnR"
   },
   "outputs": [],
   "source": [
    "def LassoVarSelection(X_train,Y_train,X_test,Y_test):\n",
    "    alpha=AlphaGridSearch(X_train,Y_train)\n",
    "    lasso = Lasso(alpha = alpha, random_state = 42)\n",
    "    lasso.fit(X_train, Y_train)\n",
    "    cols_lasso = []\n",
    "    weight_lasso = []\n",
    "    print(lasso.coef_)\n",
    "    for feature, weight in zip(X_train.columns.values, lasso.coef_):\n",
    "        if weight != 0:\n",
    "            cols_lasso.append(feature)\n",
    "            weight_lasso.append(weight)\n",
    "    \n",
    "    print(cols_lasso)\n",
    "    \n",
    "    train_X_lasso = X_train[cols_lasso]\n",
    "    test_X_lasso = X_test[cols_lasso]\n",
    "    return train_X_lasso,test_X_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "aborted",
     "timestamp": 1627135938271,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "Yf8CTwhqEhnS"
   },
   "outputs": [],
   "source": [
    "def RidgeVarSelection(X_train, Y_train, X_test, Y_test):\n",
    "      alpha = AlphaGridSearch(X_train, Y_train)\n",
    "      ridge = Ridge(alpha = alpha, random_state = 42)\n",
    "      ridge.fit(X_train, Y_train)\n",
    "      cols_ridge = []\n",
    "      weight_ridge = []\n",
    "      for feature, weight in zip(X_train.columns.values, ridge.coef_):\n",
    "        if weight != 0:\n",
    "            cols_ridge.append(feature)\n",
    "            weight_ridge.append(weight)\n",
    "            \n",
    "      print(cols_ridge)\n",
    "      train_X_ridge = X_train[cols_ridge]\n",
    "      test_X_ridge = X_test[cols_ridge]\n",
    "      return train_X_ridge, test_X_ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "aborted",
     "timestamp": 1627135938272,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "ypJesXDCEhnT"
   },
   "outputs": [],
   "source": [
    "def ElasticNetVarSelection(X_train, Y_train, X_test, Y_test):\n",
    "    lambda_enet, gamma_enet = LGGridSearch(X_train, Y_train)\n",
    "    e_net = ElasticNet(alpha = lambda_enet, l1_ratio = gamma_enet)\n",
    "    e_net.fit(X_train, Y_train)\n",
    "    # calculate the prediction and mean square error\n",
    "    cols_enet = []\n",
    "    weight_enet = []\n",
    "    for feature, weight in zip(X_train.columns.values, e_net.coef_):\n",
    "        if weight != 0:\n",
    "            cols_enet.append(feature)\n",
    "            weight_enet.append(weight)\n",
    "    \n",
    "    train_X_enet = X_train[cols_enet]\n",
    "    test_X_enet = X_test[cols_enet]\n",
    "\n",
    "    return train_X_enet, test_X_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "aborted",
     "timestamp": 1627135938274,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "tDaf1m9EEhnT"
   },
   "outputs": [],
   "source": [
    "def RandomForest(X_train, Y_train, X_test, Y_test, param_grid, airport_nm):\n",
    "    \n",
    "    train_X_lasso,test_X_lasso=LassoVarSelection(X_train,Y_train,X_test,Y_test)\n",
    "    train_X_ridge,test_X_ridge=RidgeVarSelection(X_train,Y_train,X_test,Y_test)\n",
    "    train_X_enet,test_X_enet=ElasticNetVarSelection(X_train,Y_train,X_test,Y_test)\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = 2, verbose = 2)\n",
    "    \n",
    "    \n",
    "    # Lasso with RF\n",
    "    grid_search.fit(train_X_lasso, Y_train)\n",
    "    rf_lasso = RandomForestClassifier(**grid_search.best_params_, oob_score = True)\n",
    "    rf_lasso.fit(train_X_lasso, Y_train)\n",
    "    rf_lasso_pred = rf_lasso.predict(test_X_lasso)\n",
    "    rf_lasso_prob = rf_lasso.predict_proba(test_X_lasso)\n",
    "    lld_lasso = np.sum(rf_lasso_prob[:,4:].mean(axis = 0))\n",
    "    filename = 'rf_lasso_{}.sav'.format(airport_nm)\n",
    "    pickle.dump(rf_lasso, open(filename, 'wb'))\n",
    "    accuracy= display_metrics(filename, rf_lasso_pred,Y_test)\n",
    "    models_dct[filename]={'airport':airport_nm, 'features':list(train_X_lasso.columns), 'accuracy': accuracy, 'LL Delayed':lld_lasso}\n",
    "    \n",
    "    # Ridge with RF\n",
    "    rf_ridge = RandomForestClassifier(**grid_search.best_params_, oob_score = True)\n",
    "    rf_ridge.fit(train_X_ridge, Y_train)\n",
    "    rf_ridge_pred = rf_ridge.predict(test_X_ridge)\n",
    "    rf_ridge_prob = rf_ridge.predict_proba(test_X_ridge)\n",
    "    lld_ridge = np.sum(rf_ridge_prob[:,4:].mean(axis = 0))\n",
    "    filename = 'rf_ridge_{}.sav'.format(airport_nm)\n",
    "    pickle.dump(rf_ridge, open(filename, 'wb'))\n",
    "    accuracy= display_metrics(filename, rf_ridge_pred,Y_test)\n",
    "    models_dct[filename]={'airport':airport_nm, 'features':list(train_X_ridge.columns), 'accuracy': accuracy, 'LL Delayed':lld_ridge}\n",
    "    \n",
    "    # Enet with RF\n",
    "    rf_enet = RandomForestClassifier(**grid_search.best_params_, oob_score = True)\n",
    "    rf_enet.fit(train_X_enet, Y_train)\n",
    "    rf_enet_pred = rf_enet.predict(test_X_enet)\n",
    "    rf_enet_prob = rf_enet.predict_proba(test_X_enet)\n",
    "    lld_enet = np.sum(rf_enet_prob[:,4:].mean(axis = 0))\n",
    "    filename = 'rf_enet_{}.sav'.format(airport_nm)\n",
    "    pickle.dump(rf_enet, open(filename, 'wb'))\n",
    "    accuracy= display_metrics(filename, rf_enet_pred,Y_test)\n",
    "    models_dct[filename]={'airport':airport_nm, 'features':list(train_X_enet.columns), 'accuracy': accuracy, 'LL Delayed':lld_enet}\n",
    "    return models_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "aborted",
     "timestamp": 1627135938275,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "6o2ANXa_EhnT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "def combine_files():\n",
    "    filePath= '/Users/jinhaenglee/Desktop/Georgia_Tech_OMSA/Practicum/Data/'\n",
    "    i=0\n",
    "    dct={}\n",
    "    for file in os.listdir(filePath):\n",
    "        f=file\n",
    "        if f.endswith('.xlsx'):\n",
    "            tmpDF=pd.read_excel(open(filePath+'/'+f, 'rb'),sheet_name='Data',header=1)\n",
    "            tmpDF['airportNM']=f[0:3]\n",
    "            dct[f]=tmpDF\n",
    "    \n",
    "    df_lst=[]\n",
    "    for k,v in dct.items():\n",
    "        df_lst.append(v)\n",
    "    dataDF= pd.concat(df_lst)\n",
    "    dataDF.rename(columns={dataDF.columns[15]: 'WXCodes'},inplace=True)\n",
    "    dataDF.rename(columns={dataDF.columns[22]: 'SchedDepart'},inplace=True)\n",
    "    dataDF.rename(columns={dataDF.columns[23]: 'SchedArriv'},inplace=True)\n",
    "    \n",
    "    return dataDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "aborted",
     "timestamp": 1627135938276,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "Mbkzy2iREhnU"
   },
   "outputs": [],
   "source": [
    "def dataprep(df,airport):\n",
    "    print(df.columns)\n",
    "    #new_header = df.iloc[0]\n",
    "    #df = df[1:].reset_index(drop = True)\n",
    "    #df.columns = new_header\n",
    "    #.rename(columns = df.iloc[0])\n",
    "    y = df['Avg Gate Arrival Delay']\n",
    "    df = df.iloc[:, :24] ## Take Columns A-X\n",
    "    df['Avg Gate Arrival Delay'] = y\n",
    "    df = df.rename(columns = {'WX\\nCodes':'WXCodes', 'Sched.\\nDepart': 'SchedDepartures', 'Sched.\\nArriv': 'SchedArriavls'})\n",
    "    ## Removing Nulls\n",
    "\n",
    "    # It would make sense to remove null rows for Sced. Departures and Sched Arrivals as we Can't really replace them with anything.\n",
    "    \n",
    "    df = df[~df['SchedDepart'].isnull()]\n",
    "\n",
    "    #1. There are common null values for METAR data. Only 24 hour of data provided for METAR. Remove these rows after midnight\n",
    "    # here I am taking Vis column but any other METAR column shares same NaNs after midnight.\n",
    "    df = df[~df['Vis'].isnull()]\n",
    "\n",
    "    # Sky 1 NaN values \n",
    "    df = df[~df['Sky 1'].isnull()]\n",
    "\n",
    "    # Dealing with NAN\n",
    "    # Recall Wind Gust Finite values are excessive wind speed from Col F. Null values are steady state. Fillna with zeros as there are no excessive wind\n",
    "    df['Wind Gust'] = df['Wind Gust'].fillna(0)\n",
    "\n",
    "    # Similarly Peak Gust Speed's NaN can be treated as 0.\n",
    "    df['Peak Gust Speed'] = df['Peak Gust Speed'].fillna(0)\n",
    "\n",
    "    # Layer 1 is NaN if Sky 1 is CLR (No Clouds below 12000 ft). Let's fill na with max(Layer 1)\n",
    "    df['Layer 1'].fillna(df['Layer 1'].max(), inplace = True)\n",
    "\n",
    "    df['Ice 1 Hour'].fillna(0, inplace = True)\n",
    "    df['Ice 3 Hour'].fillna(0, inplace = True)\n",
    "    df['Ice 6 Hour'].fillna(0, inplace = True)\n",
    "\n",
    "    # drop unrelevant columns\n",
    "    df.drop(columns = ['Time', 'Layer 2', 'Layer 3', 'Sky 2', 'Sky 3', 'Feels Like', 'Ice 1 Hour', 'Ice 3 Hour', 'Ice 6 Hour'], inplace = True)\n",
    "\n",
    "    # Deal with nulls in Sky 1\n",
    "    # Deal with WxCodes\n",
    "    df['WXCodes'].fillna('Regular', inplace = True)\n",
    "\n",
    "    # Converting Objects to Floats\n",
    "    for col in df.columns:\n",
    "        if col != 'Sky 1' and col != 'WXCodes' and col != 'Peak Gust Direction':\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Save response Variable seperately and remove from dataframe\n",
    "    df['y'] = df['Avg Gate Arrival Delay'].div(10).apply(np.floor)\n",
    "    y = df['y']\n",
    "    #print(y)\n",
    "    del df['Avg Gate Arrival Delay']\n",
    "    del df['y']\n",
    "\n",
    "    # Bucketting Wind Direction and Peak Gust WindDirection\n",
    "\n",
    "    df.loc[(df['Peak Gust Direction'] >= 0) & (df['Peak Gust Direction'] < 11.25),'PGDirectionBkt' ]='N'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 11.25) & (df['Peak Gust Direction'] < 33.75),'PGDirectionBkt' ]='NNE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 33.75) & (df['Peak Gust Direction'] < 56.25),'PGDirectionBkt' ]='NE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 56.25) & (df['Peak Gust Direction'] < 78.75),'PGDirectionBkt' ]='ENE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 78.75) & (df['Peak Gust Direction'] < 101.25),'PGDirectionBkt' ]='E'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 101.25) & (df['Peak Gust Direction'] < 121.75),'PGDirectionBkt' ]='ESE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 123.75) & (df['Peak Gust Direction'] < 146.25),'PGDirectionBkt' ]='SE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 146.25) & (df['Peak Gust Direction'] < 168.75),'PGDirectionBkt' ]='SSE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 168.75) & (df['Peak Gust Direction'] < 191.25),'PGDirectionBkt' ]='S'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 191.25) & (df['Peak Gust Direction'] < 213.75),'PGDirectionBkt' ]='SSW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 213.75) & (df['Peak Gust Direction'] < 236.25),'PGDirectionBkt' ]='SW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 236.25) & (df['Peak Gust Direction'] < 258.75),'PGDirectionBkt' ]='WSW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 258.75) & (df['Peak Gust Direction'] < 281.25),'PGDirectionBkt' ]='W'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 281.25) & (df['Peak Gust Direction'] < 303.75),'PGDirectionBkt' ]='WNW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 303.75) & (df['Peak Gust Direction'] < 326.25),'PGDirectionBkt' ]='NW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 326.25) & (df['Peak Gust Direction'] < 348.75),'PGDirectionBkt' ]='NNW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 348.75) & (df['Peak Gust Direction'] < 360),'PGDirectionBkt' ]='N'\n",
    "\n",
    "    # Deal with Peak Gust Direction\n",
    "    df['PGDirectionBkt']=df['PGDirectionBkt'].fillna('NA')\n",
    "    del df['Peak Gust Direction']\n",
    "    \n",
    "    \n",
    "\n",
    "    finalDf = df.copy()\n",
    "    pgdir_dct={}\n",
    "    \n",
    "    # Converting Objects to Floats\n",
    "    #for col in finalDf.columns:\n",
    "     #   finalDf[col] = finalDf[col].astype(float)\n",
    "     \n",
    "    continuous_columns=['Temp', 'Dew', 'T/D Spread', 'Wind Direction', \n",
    "                        'Wind Speed','Wind Gust', 'Altimeter', 'Vis', 'Layer 1', 'Peak Gust Speed', 'SchedDepart','SchedArriv']\n",
    "    \n",
    "    \n",
    "    #categorical_columns=['Sky 1', 'WXCodes', 'PGDirectionBkt']\n",
    "    \n",
    "    print(\"******************************Input to Encoder******************************\")\n",
    "    print(finalDf.columns)\n",
    "    \n",
    "    encoder = BinaryEncoder().fit(finalDf)\n",
    "    #encoder = OneHotEncoder().fit(finalDf)\n",
    "    finalDf_enc=encoder.transform(finalDf)\n",
    "    print(\"******************************Output of Encoder******************************\")\n",
    "    print(finalDf_enc.columns)\n",
    "    \n",
    "    \n",
    "    categorical_columns=[col for col in finalDf_enc.columns if col not in continuous_columns]\n",
    "    \n",
    "    df_corr = finalDf_enc[continuous_columns].corr(method='pearson', min_periods=1)\n",
    "    df_not_correlated = ~(df_corr.mask(np.tril(np.ones([len(df_corr)]*2, dtype=bool))).abs() > 0.8).any()\n",
    "    #print(df_not_correlated.columns)\n",
    "    un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index\n",
    "    fnl_col_lst=list(un_corr_idx)\n",
    "    fnl_col_lst+=categorical_columns\n",
    "    finalDf_enc = finalDf_enc[fnl_col_lst]\n",
    "    \n",
    "    \n",
    "   \n",
    "    print(finalDf_enc.columns)\n",
    "    filename = 'encoder_{}.sav'.format(airport)\n",
    "    pickle.dump(encoder, open(filename, 'wb'))\n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "    pgdl = finalDf['PGDirectionBkt'].drop_duplicates()\n",
    "    for i,d in enumerate(pgdl):\n",
    "        pgdir_dct[d]=i\n",
    "    finalDf.replace({\"PGDirectionBkt\": pgdir_dct},inplace=True)\n",
    "    WXDir={}\n",
    "    wxl = finalDf['WXCodes'].drop_duplicates()\n",
    "    for i,d in enumerate(wxl):\n",
    "        WXDir[d]=i\n",
    "    finalDf.replace({\"WXCodes\": WXDir},inplace=True)\n",
    "\n",
    "    # Get dummies for Sky 1\n",
    "    dummy = pd.get_dummies(finalDf['Sky 1'])\n",
    "    finalDf = pd.concat([finalDf, dummy], axis = 1)\n",
    "    del finalDf['Sky 1']\n",
    "    '''\n",
    "\n",
    "    \n",
    "    X = finalDf_enc.copy()    \n",
    "    print(X.columns)\n",
    "    \n",
    "    \n",
    "    # Bucket response Variable into intervals of 10 minutes\n",
    "    #bi_y = np.zeros((len(y),1))\n",
    "    #intervals = np.arange(10, y.max(), 10)\n",
    "    #for i in range(1, len(intervals)):\n",
    "    #    bi_y[(y > intervals[i]) & (y <= intervals[i] + 10)] = i\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # OverSample due to Imbalance Data\n",
    "    \n",
    "    \n",
    "    #y = preprocessing.label_binarize(y)\n",
    "    #print(y)\n",
    "    #y = LabelEncoder().fit_transform(y)\n",
    "    # transform the dataset\n",
    "    #oversample = SMOTE()\n",
    "    #X_ros, y_ros = oversample.fit_resample(X, y)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state= 42)\n",
    "\n",
    "    # fit predictor and target variable\n",
    "    X_ros, y_ros = ros.fit_resample(X, y)\n",
    "\n",
    "    print('Original dataset shape', len(y))\n",
    "    print('Resample dataset shape', len(y_ros))\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    finalDf = pd.DataFrame(X_ros, columns = X.columns)\n",
    "    \n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(X_ros, y_ros, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "aborted",
     "timestamp": 1627135938484,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "I11guMvhEhnV"
   },
   "outputs": [],
   "source": [
    "def SelectBestToJson(models_dct, airport_nm):\n",
    "    filtered_d = {k:v for (k,v) in models_dct.items() if airport_nm in v['airport']}\n",
    "    a =sorted(filtered_d.items(),key=lambda x: x[1]['accuracy'],reverse=True)[:1]\n",
    "    dict(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "aborted",
     "timestamp": 1627135938485,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "qy_wSKDfEhnV"
   },
   "outputs": [],
   "source": [
    "def train(flag):\n",
    "    \n",
    "    param_grid = {'bootstrap': [True],\n",
    "                   'max_depth': [50,70,100],\n",
    "                    'max_features': [5],\n",
    "                    'min_samples_leaf': [3],\n",
    "                    'min_samples_split': [8],\n",
    "                    'n_estimators': [50, 100, 200]}\n",
    "    \n",
    "\n",
    "        \n",
    "    dataDF = combine_files()\n",
    "    if flag == 0:\n",
    "      airports=list(dataDF['airportNM'].unique())    \n",
    "    elif flag == 1:\n",
    "      airports = ['ATL']\n",
    "    \n",
    "    \n",
    "    for airport in airports:\n",
    "        print('Starting data prep for',airport)\n",
    "        airportdf=dataDF[(dataDF['airportNM']==airport)]\n",
    "        train_X, test_X, train_y, test_y = dataprep(airportdf,airport)\n",
    "        print('Finished data prep for',airport)\n",
    "        models_dct = RandomForest(train_X, train_y,test_X,test_y,param_grid,airport)\n",
    "        finalModels = SelectBestToJson(models_dct, airport)\n",
    "        print('Model training completed')\n",
    "        \n",
    "    \n",
    "        filename = 'rf_{}.json'.format(airport)\n",
    "        with open(filename, \"w\") as outfile:\n",
    "            json.dump(finalModels, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "aborted",
     "timestamp": 1627135938486,
     "user": {
      "displayName": "John Lee",
      "photoUrl": "",
      "userId": "08437148378827365048"
     },
     "user_tz": 300
    },
    "id": "ZCmlRNkQEhnW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data prep for ATL\n",
      "Index(['Time', 'Temp', 'Dew', 'T/D Spread', 'Wind Direction', 'Wind Speed',\n",
      "       'Wind Gust', 'Altimeter', 'Vis', 'Sky 1', 'Sky 2', 'Sky 3', 'Layer 1',\n",
      "       'Layer 2', 'Layer 3', 'WXCodes', 'Ice 1 Hour', 'Ice 3 Hour',\n",
      "       'Ice 6 Hour', 'Peak Gust Speed', 'Peak Gust Direction', 'Feels Like',\n",
      "       'SchedDepart', 'SchedArriv', 'Actual\\nDepartures', 'Actual\\nArrivals',\n",
      "       '% On-Time Gate Departures', '% On-Time Airport Departures',\n",
      "       '% On-Time Gate Arrivals', 'Avg Gate Departure Delay',\n",
      "       'Avg Taxi Out Time', 'Avg Taxi Out Delay', 'AvgAirport Departure Delay',\n",
      "       'Avg Airborne Delay', 'Avg Taxi In Delay', 'Avg Block Delay',\n",
      "       'Avg Gate Arrival Delay', 'Ceiling', 'Weather Missing',\n",
      "       'Perform Missing', 'Either Missing', 'Good Weather', 'Delayed <XX min',\n",
      "       'Over\\nNight', 'Ignore', 'Speed', 'Gust', 'Vis.1', 'Ceil', 'Comb',\n",
      "       'airportNM'],\n",
      "      dtype='object')\n",
      "******************************Input to Encoder******************************\n",
      "Index(['Temp', 'Dew', 'T/D Spread', 'Wind Direction', 'Wind Speed',\n",
      "       'Wind Gust', 'Altimeter', 'Vis', 'Sky 1', 'Layer 1', 'WXCodes',\n",
      "       'Peak Gust Speed', 'SchedDepart', 'SchedArriv', 'PGDirectionBkt'],\n",
      "      dtype='object')\n",
      "******************************Output of Encoder******************************\n",
      "Index(['Temp', 'Dew', 'T/D Spread', 'Wind Direction', 'Wind Speed',\n",
      "       'Wind Gust', 'Altimeter', 'Vis', 'Sky 1_0', 'Sky 1_1', 'Sky 1_2',\n",
      "       'Sky 1_3', 'Layer 1', 'WXCodes_0', 'WXCodes_1', 'WXCodes_2',\n",
      "       'WXCodes_3', 'WXCodes_4', 'WXCodes_5', 'Peak Gust Speed', 'SchedDepart',\n",
      "       'SchedArriv', 'PGDirectionBkt_0', 'PGDirectionBkt_1',\n",
      "       'PGDirectionBkt_2', 'PGDirectionBkt_3', 'PGDirectionBkt_4'],\n",
      "      dtype='object')\n",
      "Index(['Temp', 'T/D Spread', 'Wind Direction', 'Wind Speed', 'Wind Gust',\n",
      "       'Altimeter', 'Vis', 'Layer 1', 'Peak Gust Speed', 'SchedDepart',\n",
      "       'Sky 1_0', 'Sky 1_1', 'Sky 1_2', 'Sky 1_3', 'WXCodes_0', 'WXCodes_1',\n",
      "       'WXCodes_2', 'WXCodes_3', 'WXCodes_4', 'WXCodes_5', 'PGDirectionBkt_0',\n",
      "       'PGDirectionBkt_1', 'PGDirectionBkt_2', 'PGDirectionBkt_3',\n",
      "       'PGDirectionBkt_4'],\n",
      "      dtype='object')\n",
      "Index(['Temp', 'T/D Spread', 'Wind Direction', 'Wind Speed', 'Wind Gust',\n",
      "       'Altimeter', 'Vis', 'Layer 1', 'Peak Gust Speed', 'SchedDepart',\n",
      "       'Sky 1_0', 'Sky 1_1', 'Sky 1_2', 'Sky 1_3', 'WXCodes_0', 'WXCodes_1',\n",
      "       'WXCodes_2', 'WXCodes_3', 'WXCodes_4', 'WXCodes_5', 'PGDirectionBkt_0',\n",
      "       'PGDirectionBkt_1', 'PGDirectionBkt_2', 'PGDirectionBkt_3',\n",
      "       'PGDirectionBkt_4'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinhaenglee/anaconda3/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape 8658\n",
      "Resample dataset shape 126702\n",
      "Finished data prep for ATL\n",
      "Performing Grid Searching for best alpha...\n",
      "[ 2.58385567e-01 -1.61607554e-01  6.40568840e-03 -3.22222941e-01\n",
      "  4.69740575e-02  1.11186769e+01 -3.00735010e-01 -6.05204164e-07\n",
      " -7.14448776e-02 -1.48583244e-01  0.00000000e+00 -2.77622485e+00\n",
      " -2.16058050e+00 -4.41347751e+00  0.00000000e+00  4.50711313e+00\n",
      "  9.18650089e-01 -7.28595601e-01  7.85533086e-02  7.49239459e-01\n",
      "  0.00000000e+00  9.48452900e-01 -1.31356419e+00  3.03846992e+00\n",
      " -2.10269636e+00]\n",
      "['Temp', 'T/D Spread', 'Wind Direction', 'Wind Speed', 'Wind Gust', 'Altimeter', 'Vis', 'Layer 1', 'Peak Gust Speed', 'SchedDepart', 'Sky 1_1', 'Sky 1_2', 'Sky 1_3', 'WXCodes_1', 'WXCodes_2', 'WXCodes_3', 'WXCodes_4', 'WXCodes_5', 'PGDirectionBkt_1', 'PGDirectionBkt_2', 'PGDirectionBkt_3', 'PGDirectionBkt_4']\n",
      "Performing Grid Searching for best alpha...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinhaenglee/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=9.48632e-19): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Temp', 'T/D Spread', 'Wind Direction', 'Wind Speed', 'Wind Gust', 'Altimeter', 'Vis', 'Layer 1', 'Peak Gust Speed', 'SchedDepart', 'Sky 1_1', 'Sky 1_2', 'Sky 1_3', 'WXCodes_1', 'WXCodes_2', 'WXCodes_3', 'WXCodes_4', 'WXCodes_5', 'PGDirectionBkt_1', 'PGDirectionBkt_2', 'PGDirectionBkt_3', 'PGDirectionBkt_4']\n",
      "Performing Grid Searching for best gamma and labmda...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "______________________________________________\n",
      "Classifier: rf_lasso_ATL.sav\n",
      "Accuracy: 0.9917919576970128\n",
      "Precision: 0.9919903687229613\n",
      "Recall: 0.9917919576970128\n",
      "F1-score: 0.9916881562123525\n",
      "Balanced Accuracy: 0.991922722773933\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: rf_ridge_ATL.sav\n",
      "Accuracy: 0.991831419438854\n",
      "Precision: 0.9921395852454198\n",
      "Recall: 0.991831419438854\n",
      "F1-score: 0.9917309937563891\n",
      "Balanced Accuracy: 0.9919597088211967\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: rf_enet_ATL.sav\n",
      "Accuracy: 0.9916735724714889\n",
      "Precision: 0.9919316190817843\n",
      "Recall: 0.9916735724714889\n",
      "F1-score: 0.9915797134788107\n",
      "Balanced Accuracy: 0.9918058209284184\n",
      "______________________________________________\n",
      "\n",
      "Model training completed\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trainFlg='Y'\n",
    "    if trainFlg=='Y':\n",
    "        train(1)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #weatherDataDF=combine_files()\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "KnowDelay_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
