{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uKmQywr58YtB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "from imblearn.over_sampling import (RandomOverSampler, SMOTENC)\n",
    "from sklearn.linear_model import (ElasticNet, Ridge, Lasso, LogisticRegression)\n",
    "from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "import json\n",
    "from category_encoders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LcLPOe3E8YtE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xd5Ji9yR8YtF"
   },
   "outputs": [],
   "source": [
    "def display_metrics(classifierName,Y_pred,Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    if np.array_equal(Y_pred,Y_true):\n",
    "        print(\"Match\")\n",
    "        \n",
    "    acc, precision, recall, f1score,bal_accuracy = classification_metrics(Y_pred,Y_true)\n",
    "    mc_points = Y_pred==Y_true\n",
    "    mc_p=np.size(mc_points) - np.count_nonzero(mc_points)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    #print((\"AUC: \"+str(auc_)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print((\"Balanced Accuracy: \"+str(bal_accuracy)))\n",
    "    #print((\"Misclassified Points: \"+str(mc_p)))\n",
    "    \n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return acc, precision, recall, f1score, bal_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oKRlBysM8YtF"
   },
   "outputs": [],
   "source": [
    "def classification_metrics(Y_pred,Y_true):\n",
    "    cnf_matrix = metrics.confusion_matrix(Y_true, Y_pred)\n",
    "    accuracy=metrics.accuracy_score(Y_true, Y_pred)\n",
    "    bal_accuracy= metrics.balanced_accuracy_score(Y_true, Y_pred)\n",
    "    precision=metrics.precision_score(Y_true, Y_pred,average='weighted')\n",
    "    recall=metrics.recall_score(Y_true, Y_pred,average='weighted')\n",
    "    fscore=metrics.f1_score(Y_true, Y_pred,average='weighted')\n",
    "    #auc = metrics.roc_auc_score(Y_true, Y_pred,average='micro')\n",
    "    return accuracy,precision,recall,fscore,bal_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CFS1kOhu8YtG"
   },
   "outputs": [],
   "source": [
    "def dataprep(df,airport):\n",
    "    print(df.columns)\n",
    "    #new_header = df.iloc[0]\n",
    "    #df = df[1:].reset_index(drop = True)\n",
    "    #df.columns = new_header\n",
    "    #.rename(columns = df.iloc[0])\n",
    "    y = df['Avg Gate Arrival Delay']\n",
    "    df = df.iloc[:, :24] ## Take Columns A-X\n",
    "    df['Avg Gate Arrival Delay'] = y\n",
    "    df = df.rename(columns = {'WX\\nCodes':'WXCodes', 'Sched.\\nDepartures': 'SchedDepart', 'Sched.\\nArrivals': 'SchedArriv'})\n",
    "    ## Removing Nulls\n",
    "\n",
    "    # It would make sense to remove null rows for Sced. Departures and Sched Arrivals as we Can't really replace them with anything.\n",
    "    \n",
    "    df = df[~df['SchedDepart'].isnull()]\n",
    "\n",
    "    #1. There are common null values for METAR data. Only 24 hour of data provided for METAR. Remove these rows after midnight\n",
    "    # here I am taking Vis column but any other METAR column shares same NaNs after midnight.\n",
    "    df = df[~df['Vis'].isnull()]\n",
    "\n",
    "    # Sky 1 NaN values \n",
    "    df = df[~df['Sky 1'].isnull()]\n",
    "\n",
    "    # Dealing with NAN\n",
    "    # Recall Wind Gust Finite values are excessive wind speed from Col F. Null values are steady state. Fillna with zeros as there are no excessive wind\n",
    "    df['Wind Gust'] = df['Wind Gust'].fillna(0)\n",
    "\n",
    "    # Similarly Peak Gust Speed's NaN can be treated as 0.\n",
    "    df['Peak Gust Speed'] = df['Peak Gust Speed'].fillna(0)\n",
    "\n",
    "    # Layer 1 is NaN if Sky 1 is CLR (No Clouds below 12000 ft). Let's fill na with max(Layer 1)\n",
    "    df['Layer 1'].fillna(df['Layer 1'].max(), inplace = True)\n",
    "\n",
    "    df['Ice 1 Hour'].fillna(0, inplace = True)\n",
    "    df['Ice 3 Hour'].fillna(0, inplace = True)\n",
    "    df['Ice 6 Hour'].fillna(0, inplace = True)\n",
    "\n",
    "    # drop unrelevant columns\n",
    "    df.drop(columns = ['Time', 'Layer 2', 'Layer 3', 'Sky 2', 'Sky 3', 'Feels Like', 'Ice 1 Hour', 'Ice 3 Hour', 'Ice 6 Hour'], inplace = True)\n",
    "\n",
    "    # Deal with nulls in Sky 1\n",
    "    # Deal with WxCodes\n",
    "    df['WXCodes'].fillna('Regular', inplace = True)\n",
    "\n",
    "    # Converting Objects to Floats\n",
    "    for col in df.columns:\n",
    "        if col != 'Sky 1' and col != 'WXCodes' and col != 'Peak Gust Direction':\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Save response Variable seperately and remove from dataframe\n",
    "    df['y'] = df['Avg Gate Arrival Delay'].div(10).apply(np.floor)\n",
    "    y = df['y']\n",
    "    #print(y)\n",
    "    del df['Avg Gate Arrival Delay']\n",
    "    del df['y']\n",
    "\n",
    "    # Bucketting Wind Direction and Peak Gust WindDirection\n",
    "\n",
    "    df.loc[(df['Peak Gust Direction'] >= 0) & (df['Peak Gust Direction'] < 11.25),'PGDirectionBkt' ]='N'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 11.25) & (df['Peak Gust Direction'] < 33.75),'PGDirectionBkt' ]='NNE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 33.75) & (df['Peak Gust Direction'] < 56.25),'PGDirectionBkt' ]='NE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 56.25) & (df['Peak Gust Direction'] < 78.75),'PGDirectionBkt' ]='ENE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 78.75) & (df['Peak Gust Direction'] < 101.25),'PGDirectionBkt' ]='E'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 101.25) & (df['Peak Gust Direction'] < 121.75),'PGDirectionBkt' ]='ESE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 123.75) & (df['Peak Gust Direction'] < 146.25),'PGDirectionBkt' ]='SE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 146.25) & (df['Peak Gust Direction'] < 168.75),'PGDirectionBkt' ]='SSE'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 168.75) & (df['Peak Gust Direction'] < 191.25),'PGDirectionBkt' ]='S'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 191.25) & (df['Peak Gust Direction'] < 213.75),'PGDirectionBkt' ]='SSW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 213.75) & (df['Peak Gust Direction'] < 236.25),'PGDirectionBkt' ]='SW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 236.25) & (df['Peak Gust Direction'] < 258.75),'PGDirectionBkt' ]='WSW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 258.75) & (df['Peak Gust Direction'] < 281.25),'PGDirectionBkt' ]='W'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 281.25) & (df['Peak Gust Direction'] < 303.75),'PGDirectionBkt' ]='WNW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 303.75) & (df['Peak Gust Direction'] < 326.25),'PGDirectionBkt' ]='NW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 326.25) & (df['Peak Gust Direction'] < 348.75),'PGDirectionBkt' ]='NNW'\n",
    "    df.loc[(df['Peak Gust Direction'] >= 348.75) & (df['Peak Gust Direction'] < 360),'PGDirectionBkt' ]='N'\n",
    "\n",
    "    # Deal with Peak Gust Direction\n",
    "    df['PGDirectionBkt']=df['PGDirectionBkt'].fillna('NA')\n",
    "    del df['Peak Gust Direction']\n",
    "    \n",
    "    \n",
    "\n",
    "    finalDf = df.copy()\n",
    "    pgdir_dct={}\n",
    "    \n",
    "    # Converting Objects to Floats\n",
    "    #for col in finalDf.columns:\n",
    "     #   finalDf[col] = finalDf[col].astype(float)\n",
    "     \n",
    "    continuous_columns=['Temp', 'Dew', 'T/D Spread', 'Wind Direction', \n",
    "                        'Wind Speed','Wind Gust', 'Altimeter', 'Vis', 'Layer 1', 'Peak Gust Speed', 'SchedDepart','SchedArriv']\n",
    "    \n",
    "    \n",
    "    #categorical_columns=['Sky 1', 'WXCodes', 'PGDirectionBkt']\n",
    "    \n",
    "    print(\"******************************Input to Encoder******************************\")\n",
    "    print(finalDf.columns)\n",
    "    \n",
    "    filename = 'encoder_{}.sav'.format(airport)\n",
    "    with (open(filename, \"rb\")) as openfile:\n",
    "        biencoder = pickle.load(openfile)\n",
    "    encoder = biencoder.fit(finalDf)\n",
    "    #encoder = OneHotEncoder().fit(finalDf)\n",
    "    finalDf_enc=encoder.transform(finalDf)\n",
    "    print(\"******************************Output of Encoder******************************\")\n",
    "    print(finalDf_enc.columns)\n",
    "    \n",
    "    \n",
    "    categorical_columns=[col for col in finalDf_enc.columns if col not in continuous_columns]\n",
    "    \n",
    "    df_corr = finalDf_enc[continuous_columns].corr(method='pearson', min_periods=1)\n",
    "    df_not_correlated = ~(df_corr.mask(np.tril(np.ones([len(df_corr)]*2, dtype=bool))).abs() > 0.8).any()\n",
    "    #print(df_not_correlated.columns)\n",
    "    un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index\n",
    "    fnl_col_lst=list(un_corr_idx)\n",
    "    fnl_col_lst+=categorical_columns\n",
    "    finalDf_enc = finalDf_enc[fnl_col_lst]\n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    pgdl = finalDf['PGDirectionBkt'].drop_duplicates()\n",
    "    for i,d in enumerate(pgdl):\n",
    "        pgdir_dct[d]=i\n",
    "    finalDf.replace({\"PGDirectionBkt\": pgdir_dct},inplace=True)\n",
    "    WXDir={}\n",
    "    wxl = finalDf['WXCodes'].drop_duplicates()\n",
    "    for i,d in enumerate(wxl):\n",
    "        WXDir[d]=i\n",
    "    finalDf.replace({\"WXCodes\": WXDir},inplace=True)\n",
    "\n",
    "    # Get dummies for Sky 1\n",
    "    dummy = pd.get_dummies(finalDf['Sky 1'])\n",
    "    finalDf = pd.concat([finalDf, dummy], axis = 1)\n",
    "    del finalDf['Sky 1']\n",
    "    '''\n",
    "\n",
    "    \n",
    "    X = finalDf_enc.copy()    \n",
    "    print(X.columns)\n",
    "    \n",
    "    \n",
    "    # Bucket response Variable into intervals of 10 minutes\n",
    "    #bi_y = np.zeros((len(y),1))\n",
    "    #intervals = np.arange(10, y.max(), 10)\n",
    "    #for i in range(1, len(intervals)):\n",
    "    #    bi_y[(y > intervals[i]) & (y <= intervals[i] + 10)] = i\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # OverSample due to Imbalance Data\n",
    "    \n",
    "    \n",
    "    #y = preprocessing.label_binarize(y)\n",
    "    #print(y)\n",
    "    #y = LabelEncoder().fit_transform(y)\n",
    "    # transform the dataset\n",
    "    #oversample = SMOTE()\n",
    "    #X_ros, y_ros = oversample.fit_resample(X, y)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state= 42)\n",
    "\n",
    "    # fit predictor and target variable\n",
    "    X_ros, y_ros = ros.fit_resample(X, y)\n",
    "\n",
    "    print('Original dataset shape', len(y))\n",
    "    print('Resample dataset shape', len(y_ros))\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    finalDf = pd.DataFrame(X_ros, columns = X.columns)\n",
    "    \n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(X_ros, y_ros, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CH7fJ9Uc8YtL"
   },
   "outputs": [],
   "source": [
    "def test(flag):\n",
    "    jsonFile = '/Users/jinhaenglee/Downloads/rf_BestModel_allAirPorts.json'\n",
    "    with open(jsonFile) as f:\n",
    "      data = json.load(f)\n",
    "    \n",
    "    models = list(data.keys())\n",
    "    modelsDir = '/Users/jinhaenglee/Downloads/Models/'\n",
    "    encodersDir = '/Users/jinhaenglee/Downloads/Models/Encoders/'\n",
    "    inputDir = '/Users/jinhaenglee/Desktop/Georgia_Tech_OMSA/Practicum/Data/'\n",
    "    outputDir = '/Users/jinhaenglee/Downloads/'\n",
    "    files = glob.glob(inputDir + '*.xlsx')\n",
    "    airports = []\n",
    "    for f in files:\n",
    "        airports.append(f.split('/')[-1].split(' ')[0])\n",
    "\n",
    "\n",
    "    ap = []\n",
    "    acc = []\n",
    "    pre = []\n",
    "    rec = []\n",
    "    fs = []\n",
    "    bal_ac = []\n",
    "    for airport in ['ATL']:\n",
    "\n",
    "        df = pd.read_excel(inputDir + airport + ' Final.xlsx', 'Data', header = 1)\n",
    "        train_X, test_X, train_y, test_y = dataprep(df, airport)\n",
    "        if flag == 1:\n",
    "            test_X = test_X[test_y > 3]\n",
    "            test_y = test_y[test_y > 3]\n",
    "\n",
    "        for m in models:\n",
    "            if airport in m:\n",
    "                with (open(m, \"rb\")) as openfile:\n",
    "                    RF = pickle.load(openfile)\n",
    "\n",
    "                test_X_f = test_X[data[m]['features']]\n",
    "                y_pred = RF.predict(test_X_f)\n",
    "                accuracy,precision,recall,fscore,bal_accuracy = display_metrics(m, y_pred,test_y)\n",
    "                ap.append(airport)\n",
    "                acc.append(accuracy)\n",
    "                pre.append(precision)\n",
    "                rec.append(recall)\n",
    "                fs.append(fscore)\n",
    "                bal_ac.append(bal_accuracy)\n",
    "\n",
    "\n",
    "    output = pd.DataFrame([])\n",
    "    output['Airport'] = ap\n",
    "    output['Accuracy'] = acc\n",
    "    output['Recall'] = rec\n",
    "    output['Precision'] = pre\n",
    "    output['F1 score'] = fs\n",
    "    output['Balanced Accuracy'] = bal_ac\n",
    "\n",
    "    output.to_csv(outputDir + 'KnowDelay RandomForest Results.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flnmtJQ18YtM",
    "outputId": "e7b4a08d-b475-4868-abbf-1ea276377fa6"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    testFlag = 'Y'\n",
    "    if testFlag == 'Y':\n",
    "        test(1)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NZA3x0y78YtR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NAOOuHfH8YtS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryEncoder()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "KnowDelay_Testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
